# The time complexity of an algorithm is a measure of the amount of time it takes to run as a function of the size of the input. It is usually expressed using Big O notation, which describes the upper bound of the growth rate of the algorithm.
# The time complexity of an algorithm can be classified into different categories based on the growth rate of the algorithm. Some common time complexities include:
# O(1) - Constant time: The algorithm takes the same amount of time to run regardless of the size of the input. Example: accessing an element in an array by index.
# O(log n) - Logarithmic time: The algorithm takes logarithmic time to run
# O(n) - Linear time: The algorithm takes linear time to run, meaning that the time it takes to run is directly proportional to the size of the input. Example: iterating through a list.
# O(n log n) - Linearithmic time: The algorithm takes linearithmic time to run, meaning that the time it takes to run is proportional to n log n. Example: merge sort.
# O(n^2) - Quadratic time: The algorithm takes quadratic time to run, meaning that the time it takes to run is proportional to the square of the size of the input. Example: bubble sort.
# O(2^n) - Exponential time: The algorithm takes exponential time to run, meaning that the time it takes to run doubles with each additional element in the input. Example: recursive Fibonacci calculation.
# O(n!) - Factorial time: The algorithm takes factorial time to run, meaning that the time it takes to run grows factorially with the size of the input. Example: generating all permutations of a set.



# The time complexity of iterables
# 1. Lists: O(n) for searching, O(1) for accessing by index, O(n) for inserting or deleting an element.
# 2. Tuples: O(1) for searching, O(1) for accessing by index, O(n) for inserting or deleting an element. Hold onðŸ¤¨how can we remove elements of tuples?
# 3. Sets: O(1) for searching, O(1) for inserting or deleting an element.
# 4. Dictionaries: O(1) for searching, O(1) for inserting or deleting an element.